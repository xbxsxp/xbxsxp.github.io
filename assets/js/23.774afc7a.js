(window.webpackJsonp=window.webpackJsonp||[]).push([[23],{344:function(e,t,a){e.exports=a.p+"assets/img/MAE.6d06d5fa.jpg"},409:function(e,t,a){"use strict";a.r(t);var s=a(4),r=Object(s.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"mae原理"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#mae原理"}},[e._v("#")]),e._v(" MAE原理")]),e._v(" "),t("p",[e._v("MAE 的方法很简单：Mask 掉输入图像的随机的 patches 并重建它们。它基于两个核心理念：研究人员开发了一个非对称编码器 - 解码器架构，其中一个编码器只对可见的 patch 子集进行操作 (即没有被 mask 掉的 token)，另一个简单解码器可以从潜在表征和被 masked 掉的 token 重建原始图像。Decoder 的架构可以是十分轻量化的模型，且具体的架构对模型性能影响很大。研究人员进一步发现，Mask 掉大部分输入图像 (例如 75%) 会产生重要且有意义的自监督任务。结合这两种设计，我们就能高效地训练大型模型：提升训练速度至 3 倍或更多，并提高准确性。")]),e._v(" "),t("p",[e._v("MAE 和 ViT 的做法一致，将图像划分成规则的，不重叠的 patches。然后按照均匀分布不重复地选择一些 patches 并且 mask 掉剩余的 patches。作者采用的 mask ratio 足够高，因此大大减小了 patches 的冗余信息，使得在这种情况下重建 images 不那么容易。")]),e._v(" "),t("div",{staticClass:"center-container"},[t("p",[t("img",{attrs:{src:a(344),alt:"在这里插入图片描述"}})])]),t("h3",{attrs:{id:"encoder"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#encoder"}},[e._v("#")]),e._v(" Encoder")]),e._v(" "),t("p",[e._v("MAE Encoder 采用 ViT 架构，但只会作用于 unmasked images。和 ViT 思路一样，MAE Encoder 会先通过 Linear Projection 编码图片，再加上位置编码，随后送入一堆连续的 Transformer Block 里面。但是编码器只对整个图片 patches 集合的一个小子集 (例如25%)进行操作，而删除 masked patches。这里和 BERT 做法不一样，BERT 使用对于 mask 掉的部分使用特殊字符，而 MAE 不使用掩码标记。")]),e._v(" "),t("h3",{attrs:{id:"decoder"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#decoder"}},[e._v("#")]),e._v(" Decoder")]),e._v(" "),t("p",[e._v("MAE Decoder 采用 Transformer 架构，输入整个图片 patches 集合，不光是 unmasked tokens (图4中蓝色色块)，还有被 mask 掉的部分 (图4中灰色色块)。每个 mask tokens 都是一个共享的、学习的向量，它指示了这里有一个待预测的 tokens。作者还将位置嵌入添加到这个完整 image patch 集合中的所有 tokens 中，位置编码表示每个 patches 在图像中的位置的信息。")]),e._v(" "),t("p",[e._v("MAE Decoder 仅用于预训练期间执行图像重建任务。因为自监督学习的特点就是只用最后预训练好的 Encoder 完成分类任务。因此，可以灵活设计与编码器设计无关的解码器结构。作者用比编码器更窄更浅的很小的解码器做实验。 在这种非对称的设计下，tokens 就可以由轻量级解码器处理，这大大缩短了预训练的时间。")]),e._v(" "),t("h3",{attrs:{id:"自监督学习目标函数-reconstruction-target"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#自监督学习目标函数-reconstruction-target"}},[e._v("#")]),e._v(" 自监督学习目标函数 Reconstruction Target")]),e._v(" "),t("p",[e._v("Decoder 的最后一层是一个 Linear Projection 层，其输出的 channel 数等于图像的像素 (pixel) 数。所以 Decoder 的输出会进一步 reshape 成图像的形状。损失函数就是 MSE Loss，即直接让 reconstructed image 和 input image 的距离越接近越好。")]),e._v(" "),t("p",[e._v("作者还尝试了另外一种损失函数，就是先计算出每个 patch 的像素值的 mean 和 deviation，并使用它们去归一化这个 patch 的每个像素值。最后再使用归一化的像素值进行 MSE Loss 计算。但是发现这样做的效果比直接 MSE Loss 好。")]),e._v(" "),t("h3",{attrs:{id:"mae的具体实现方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#mae的具体实现方法"}},[e._v("#")]),e._v(" MAE的具体实现方法")]),e._v(" "),t("ul",[t("li",[e._v("首先通过 Linear Projection 和位置编码得到 image tokens")]),e._v(" "),t("li",[e._v("随机 shuffle 这些 tokens，按照 masking ratio 扔掉最后的一部分")]),e._v(" "),t("li",[e._v("把 unmasked patches 输出到 Encoder 中，得到这些 tokens 的表征")]),e._v(" "),t("li",[e._v("把 Encoder 的输出，结合 masked tokens (可学习的向量)，执行 unshuffle操作恢复顺序，再一起输入到 Decoder 中")]),e._v(" "),t("li",[e._v("shuffle 和 unshuffle 操作的时间开销可忽略不计")])])])}),[],!1,null,null,null);t.default=r.exports}}]);